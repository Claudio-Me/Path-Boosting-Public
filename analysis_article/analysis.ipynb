{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from analysis_article.true_positive_ratio import true_positive_ratio\n",
    "from analysis_article.paths_importance import paths_importance_analysis\n",
    "from analysis_article.cross_validation_overfitting_iteration import cross_validation\n",
    "from analysis_article.signal_to_noise import signal_to_noise\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to the accompanying Jupyter Notebook for our published paper. This interactive notebook is designed to allow you to reproduce the results presented in our study with ease. By following the instructions and running the provided code cells, you can verify the findings and explore the data and methodologies used in our research.\n",
    "\n",
    "Please be aware that due to the computational intensity of some of the analyses, the Jupyter Notebook environment may occasionally encounter difficulties in executing the entire workflow. If you encounter such issues, we have provided a robust alternative to ensure you can still replicate the results.\n",
    "\n",
    "Within the directory of this notebook, locate the function files that the Jupyter Notebook is intended to run. At the end of each of these files, there is a section of commented-out code. This code is identical to what is executed within the Jupyter Notebook. To proceed, simply uncomment this code block and run the file as a standalone script in your preferred Python environment. By doing so, you should be able to achieve the same outputs as those intended within the Jupyter Notebook setup without any compromise in the results.\n",
    "\n",
    "We hope this notebook enhances your understanding of our work, and we encourage you to reach out with any questions or feedback you may have."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction to True Positive Ratio Plotting with Synthetic Dataset Analysis\n",
    "\n",
    "In this snipped of code we will be plotting the true positive ratio using a synthetic dataset. Our objective is to gauge the efficacy of an algorithm known for its path selection capabilities.\n",
    "\n",
    "## Parameters Description:\n",
    "\n",
    "- `number_of_simulations` (Default: 200): This parameter controls the number of simulation iterations for the algorithm. To secure a robust measure of algorithm performance, we average the results over these simulations, defaulting to 200 unless adjusted according to user requirements.\n",
    "\n",
    "- `synthetic_dataset_scenario`: Selects the scenario for synthetic dataset generation, each designed to challenge the algorithm in different ways, reflecting various possible real-life conditions.\n",
    "\n",
    "## True Positive Ratio (TPR):\n",
    "The True Positive Ratio is indicative of the algorithm's accuracy, computed each iteration as the number of correct path selections over the total selected paths.\n",
    "\n",
    "## Synthetic Dataset:\n",
    "Through the `synthetic_dataset_scenario` parameter, the user can decide which setting to replicate.\n",
    "\n",
    "\n",
    "## Note on Path Boosting Methodologies:\n",
    "It is important to note the distinction in path boosting methods applied in different scenarios. For scenarios 1 and 2, where only a single metal center is considered, path boosting is employed in its standard form. However, scenario 3 is unique in that it applies cyclic path boosting, taking into account the multiple metal centers that influence the path selection process in more complex ways. This specificity in methodology is crucial to accurately modeling and analyzing each scenario's corresponding dataset.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "true_positive_ratio(number_of_simulations=200, synthetic_dataset_scenario=1, noise_variance=0.2,\n",
    "                        maximum_number_of_steps=None, save_fig=False, show_settings=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Paths Importance\n",
    "Get paths importance, settings as before, in addition consider the parameter `update_features_importance_by_comparison`, to select the importance measure\n",
    "\n",
    "## Importance measure:\n",
    "If `update_features_importance_by_comparison` is `True` then the importance of each selected column is given by the error improvment of the seclected column compared with the second best choice.\n",
    "If `update_features_importance_by_comparison` is `False` the importance of the selected column is given by the overall error improvment.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": "paths_importance_analysis(\"5k_synthetic_dataset\", number_of_simulations=200, synthetic_dataset_scenario=2, noise_variance=0.2, maximum_number_of_steps=None, update_features_importance_by_comparison=True, show_settings=True)",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cross Validation\n",
    "Performs cross validation to find the optimal `maximum_number_of_steps`.\n",
    "Patience is the number of consecutive steps where increases in the cross validation test error after which we consider the algorithm is overfitting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "cross_validation(number_of_simulations=15, k_folds=5, scenario=1, patience=3, dataset_name=\"5k_synthetic_dataset\", noise_variance=0.2, maximum_number_of_steps=None, save_fig=False, use_wrapper_boosting=None, show_settings=True)\n"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Signal to noise ratio"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "signal_to_noise(number_of_simulations=10,\n",
    "                noise_variance_list=[0.2, 0.325, 0.5, 0.625, 0.75, 0.875, 1, 1.125, 1.25, 1.375, 1.5, 1.625],\n",
    "                # [0.2, 0.325, 0.5, 0.625, 0.75, 0.875, 1, 1.125, 1.25, 1.375, 1.5, 1.625]\n",
    "                synthetic_dataset_scenario=3,\n",
    "                dataset_name=\"5k_synthetic_dataset\", noise_variance=0.2, maximum_number_of_steps=None,\n",
    "                save_fig=True, use_wrapper_boosting=None, show_settings=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
